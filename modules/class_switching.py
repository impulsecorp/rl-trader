"""
Class Switching estimator
"""

# Author: Anil Narassiguin <narassiguin.anil@gmail.com>, Haytham Elghazel

from __future__ import division

"""
Class Switching algorithm is a class of the ensemble methods for classificationthat combines the decisions of
classifiers generated by using a perturbed version of the training set where the classes of the training examples are
randomly switched.The classifiers generated by this proce- dure have statistically uncorrelated errors in the training
set. Hence, the ensembles they form exhibit a similar dependence of the training error on ensemble size, in- dependently
of the classification problem.
References
----------
.. [1] Martinez-Munoz, G. and Suarez, A., "Switching class labels to generate classification ensembles" (Pattern
       Recognition, 2005)
"""

import itertools
import numpy as np
import random
from warnings import warn

from sklearn.utils import check_random_state, check_X_y, check_array, column_or_1d
from sklearn.utils.validation import check_is_fitted
from sklearn.utils.multiclass import check_classification_targets
from sklearn.externals.joblib import Parallel, delayed

from sklearn.ensemble.base import BaseEnsemble, _partition_estimators
from sklearn.tree import DecisionTreeClassifier

from abc import ABCMeta
from sklearn.externals.six import with_metaclass

MAX_INT = np.iinfo(np.int32).max

def _switch_classes(y, p_switch, classes, random_state=None):
    """Private function used to switch the classes in a dataset"""
    # Set the seed for the switching procedure.
    random.seed(random_state)
    y_switch = []
    for value in y:
        if random.random() < p_switch:
            classes_copy = np.delete(np.copy(classes), value)
            new_y = random.choice(classes_copy,)
            y_switch.append(new_y)
        else:
            y_switch.append(value)

    return np.array(y_switch)

def _parallel_build_estimators(n_estimators, ensemble, X, y, seeds, verbose):
    """Private function used to build a batch of class switching estimators within a job."""
    # Retrieve ensemble settings
    p_switch = ensemble.p_switch
    n_classes = ensemble.n_classes_
    classes = ensemble.classes_

    # The maximum value of p, cf. the article
    p_max = (n_classes-1)/n_classes

    if p_switch >= p_max:
        raise ValueError("The switch ratio is too big ! Please decrease it.")

    # Build estimators
    estimators = []
    estimators_switches = []

    for i in range(n_estimators):
        if verbose > 1:
            print("building estimator %d of %d" % (i + 1, n_estimators))

        random_state = check_random_state(seeds[i])
        seed = random_state.randint(MAX_INT)
        estimator = ensemble._make_estimator(append=False)

        try:  # Not all estimator accept a random_state
            estimator.set_params(random_state=seed)
        except ValueError:
            pass

        y_switch = _switch_classes(y, p_switch, classes, random_state=random_state)
        estimator.fit(X, y_switch)

        estimators.append(estimator)
        estimators_switches.append(y_switch)

    return estimators, estimators_switches

def _parallel_predict_proba(estimators, X, n_classes):
    """Private function used to compute (proba-)predictions within a job."""
    n_samples = X.shape[0]
    proba = np.zeros((n_samples, n_classes))

    for estimator in estimators:
        if hasattr(estimator, "predict_proba"):
            proba_estimator = estimator.predict_proba(X)
            if len(estimator.classes_) > 1:
                if n_classes == len(estimator.classes_):
                    proba += proba_estimator

                else:
                    proba[:, estimator.classes_] += \
                        proba_estimator[:, range(len(estimator.classes_))]
            # Handle the case where there's only one class in the resulting training set.
            else:
                only_class = estimator.predict(X[0,:])
                if only_class == 1:
                    proba[:, 1] += 1
                else:
                    proba[:, 0] += 1

        else:
            # Resort to voting
            predictions = estimator.predict(X)

            for i in range(n_samples):
                proba[i, predictions[i]] += 1

    return proba

class ClassSwitching(with_metaclass(ABCMeta, BaseEnsemble)):
    """
    Parameters
    ----------
    p_switch : The class switching rate. Ex: if p_switch = 0.1, 10% of the classes will be randomly switched to another
               class.

    For the other parameters, please refer to scikit-learn's bagging documentation :
    https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/bagging.py
    """
    def __init__(self,
                 base_estimator=None,
                 n_estimators=10,
                 random_state=None,
                 warm_start=False,
                 verbose=0,
                 n_jobs=1,
                 p_switch=0.1):

        super(ClassSwitching, self).__init__(
            base_estimator=base_estimator,
            n_estimators=n_estimators
            )
        self.p_switch=p_switch
        self.random_state=random_state
        self.warm_start=warm_start
        self.verbose=verbose
        self.n_jobs=n_jobs

    def fit(self, X, y, sample_weight=None):
        # classes_ = self.classes_
        random_state = check_random_state(self.random_state)
        # Convert data
        X, y = check_X_y(X, y, ['csr', 'csc'])

        # Remap output
        n_samples, self.n_features_ = X.shape
        y = self._validate_y(y)

        # Check parameters
        self._validate_estimator()

        if not self.warm_start or len(self.estimators_) == 0:
            # Free allocated memory, if any
            self.estimators_ = []
            self.estimators_switches_ = []

        n_more_estimators = self.n_estimators - len(self.estimators_)

        if n_more_estimators < 0:
            raise ValueError('n_estimators=%d must be larger or equal to '
                             'len(estimators_)=%d when warm_start==True'
                             % (self.n_estimators, len(self.estimators_)))

        elif n_more_estimators == 0:
            warn("Warm-start fitting without increasing n_estimators does not "
                 "fit new trees.")
            return self

        # Parallel loop
        n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators,
                                                             self.n_jobs)

        # Advance random state to state after training
        # the first n_estimators
        if self.warm_start and len(self.estimators_) > 0:
            random_state.randint(MAX_INT, size=len(self.estimators_))

        seeds = random_state.randint(MAX_INT, size=n_more_estimators)

        all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose)(
            delayed(_parallel_build_estimators)(
                n_estimators[i],
                self,
                X,
                y,
                seeds[starts[i]:starts[i + 1]],
                verbose=self.verbose)
            for i in range(n_jobs))

        # Reduce
        self.estimators_ += list(itertools.chain.from_iterable(
            t[0] for t in all_results))
        self.estimators_switches_ += list(itertools.chain.from_iterable(
            t[1] for t in all_results))

        return self

    def predict_proba(self, X):

        check_is_fitted(self, "classes_")
        # Check data
        X = check_array(X, accept_sparse=['csr', 'csc'])

        if self.n_features_ != X.shape[1]:
            raise ValueError("Number of features of the model must "
                             "match the input. Model n_features is {0} and "
                             "input n_features is {1}."
                             "".format(self.n_features_, X.shape[1]))

        # Parallel loop
        n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators,
                                                             self.n_jobs)

        all_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose)(
            delayed(_parallel_predict_proba)(
                self.estimators_[starts[i]:starts[i + 1]],
                X,
                self.n_classes_)
            for i in range(n_jobs))

        # Reduce
        proba = sum(all_proba) / self.n_estimators

        return proba

    def predict(self, X):
        predicted_probabilitiy = self.predict_proba(X)
        return self.classes_.take((np.argmax(predicted_probabilitiy, axis=1)),
                                  axis=0)

    def _validate_estimator(self):
        super(ClassSwitching, self)._validate_estimator(
            default=DecisionTreeClassifier())

    def _validate_y(self, y):
        y = column_or_1d(y, warn=True)
        check_classification_targets(y)
        self.classes_, y = np.unique(y, return_inverse=True)
        self.n_classes_ = len(self.classes_)

        return y



