import numpy as np
import keras
from keras.datasets import mnist
from keras.layers import Input, Dense, Dropout, Conv2D, Flatten, MaxPooling2D
from keras.models import Model
from keras import backend as K
from keras.preprocessing.image import ImageDataGenerator

import time
class ForwardNN(object):
    def __init__(self):
        pass
    def fit(self,X,Y):
        batch_size = 128
        num_classes = len(np.unique(Y))
        epochs1 = 1
        epochs2 = 1
        epochs3 = 98

        train_begin_time = time.time()
        best_score = 0
        # input dimensions
        img_rows, img_cols = X.shape[1], 1
        if K.image_data_format() == 'channels_first':
            x_train = X.reshape(X.shape[0], 1, img_rows, img_cols)
            input_shape = (1, img_rows, img_cols)
        else:
            x_train = X.reshape(X.shape[0], img_rows, img_cols, 1)
            input_shape = (img_rows, img_cols, 1)
        y_train = keras.utils.to_categorical(Y)
        conv1_weights = layer1(input_shape,x_train,y_train)
        conv2_weights = layer2(conv1_weights,input_shape,x_train,y_train)
        conv3_weights,self.model = layer3(conv1_weights,conv2_weights,input_shape,x_train,y_train)
    def predict_proba(self,X_Test):
        img_rows, img_cols = X_Test.shape[1], 1
        if K.image_data_format() == 'channels_first':
            X_Test = X_Test.reshape(X_Test.shape[0], 1, img_rows, img_cols)
        else:
            X_Test = X_Test.reshape(X_Test.shape[0], img_rows, img_cols, 1)
        return self.model.predict(X_Test)
    def predict(self,X_Test):
        img_rows, img_cols = X_Test.shape[1], 1
        if K.image_data_format() == 'channels_first':
            X_Test = X_Test.reshape(X_Test.shape[0], 1, img_rows, img_cols)
        else:
            X_Test = X_Test.reshape(X_Test.shape[0], img_rows, img_cols, 1)
        return np.argmax(self.model.predict(X_Test),axis=1)
        

def save_weights(model, filename, layer):
    conv1 = model.get_layer('conv{0}'.format(layer)).get_weights()
    fc1 = model.get_layer('fc1').get_weights()
    fc2 = model.get_layer('fc2').get_weights()

    np.savez(filename, W_conv=conv1[0], b_conv=conv1[1], W_fc1=fc1[0], b_fc1=fc1[1],
                W_fc2=fc2[0], b_fc2=fc2[1])

    

train_begin_time = time.time()
best_score = 0
class TimeHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.times = []
        self.epoch_times = []
        self.best_weights = None

    def on_epoch_begin(self, epoch, logs={}):
        self.t0 = time.time()

    def on_epoch_end(self, epoch, logs={}):
        global best_score
        self.times.append(time.time() - train_begin_time)
        self.epoch_times.append(time.time() - self.t0)

        if logs.get('val_acc') > best_score:
            try:
                best_score = logs.get('val_acc')
                self.best_weights = save_weights(self.model, 'weights_layer3.npz', 3)
            except Exception:
                pass

def layer1(input_shape,x_train,y_train):
    main_input = Input(shape=input_shape, name='main_input')
    conv1 = Conv2D(256,(3,3), activation='relu', padding='same', name='conv1')(main_input)
    conv1 = MaxPooling2D(pool_size=(2,1))(conv1)
    conv1_drop = Dropout(.3)(conv1)
    conv1_flat = Flatten()(conv1_drop)
    fc1 = Dense(150, activation='relu', name='fc1')(conv1_flat)
    fc1_drop = Dropout(.5)(fc1)
    main_output = Dense(y_train.shape[1], activation='softmax', name='fc2')(fc1_drop)

    model = Model(inputs=[main_input], outputs=[main_output])
    model.compile(optimizer=keras.optimizers.Adam(lr=0.005), loss='categorical_crossentropy', metrics=['accuracy'])

    print('Using real-time data augmentation.')
    

    time_history = TimeHistory()
    history = keras.callbacks.History()
    # Fit the model on the batches generated by datagen.flow().
    model.fit(x_train, y_train, batch_size=32,
    					epochs=1, callbacks=[history, time_history],
    					validation_split=0.05)

    np.savez('layer1_mnist_results.npz', acc=history.history['acc'], loss=history.history['loss'],
    		val_acc=history.history['val_acc'], val_loss=history.history['val_loss'],
    		times=time_history.times, epoch_times=time_history.epoch_times)
    conv1_weights = model.get_layer('conv1').get_weights()

    save_weights(model, "weights_layer1.npz", 1)

    return conv1_weights

def layer2(conv1_weights,input_shape,x_train,y_train):
    main_input = Input(shape=input_shape, name='main_input')

    conv1 = Conv2D(256, (3,3), activation='relu', padding='same', trainable=False, name='conv1')(main_input)
    conv2 = Conv2D(256, (3,3), activation='relu', padding='same', name='conv2')(conv1)

    conv2 = MaxPooling2D(pool_size = (2,1))(conv2)
    conv2_drop = Dropout(.3)(conv2)
    conv2_flat = Flatten()(conv2_drop)

    fc1 = Dense(150, activation='relu', name='fc1')(conv2_flat)
    fc1_drop = Dropout(.5)(fc1)
    main_output = Dense(y_train.shape[1], activation='softmax', name='fc2')(fc1_drop)

    model = Model(inputs=[main_input], outputs=[main_output])
    model.compile(optimizer=keras.optimizers.Adam(lr=0.005), loss='categorical_crossentropy', metrics=['accuracy'])
    model.get_layer('conv1').set_weights(conv1_weights)

    time_history = TimeHistory()
    history = keras.callbacks.History()

    print('Using real-time data augmentation.')
    

    time_history = TimeHistory()
    history = keras.callbacks.History()
    # Fit the model on the batches generated by datagen.flow().
    model.fit(x_train, y_train, batch_size=32,
    					
    					epochs=1, callbacks=[history, time_history],
    					validation_split=0.05)

    np.savez('layer2_mnist_results.npz', acc=history.history['acc'], loss=history.history['loss'],
    		val_acc=history.history['val_acc'], val_loss=history.history['val_loss'],
    		times=time_history.times, epoch_times=time_history.epoch_times)
    conv2_weights = model.get_layer('conv2').get_weights()

    save_weights(model, "weights_layer2.npz", 2)

    return conv2_weights

def layer3(conv1_weights, conv2_weights,input_shape,x_train,y_train):
    main_input = Input(shape=input_shape, name='main_input')
    conv1 = Conv2D(256, (3,3), activation='relu', padding='same', trainable=False, name='conv1')(main_input)
    conv2 = Conv2D(256, (3,3), activation='relu', padding='same', trainable=False, name='conv2')(conv1)

    conv2 = MaxPooling2D(pool_size = (2,1))(conv2)

    conv3 = Conv2D(128, (3,3), activation='relu', padding='same', name='conv3')(conv2)
    conv3_drop = Dropout(.3)(conv3)
    conv3_flat = Flatten()(conv3_drop)

    fc1 = Dense(150, activation='relu', name='fc1')(conv3_flat)
    fc1_drop = Dropout(.5)(fc1)
    main_output = Dense(y_train.shape[1], activation='softmax', name='fc2')(fc1_drop)

    model = Model(inputs=[main_input], outputs=[main_output])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    model.get_layer('conv1').set_weights(conv1_weights)
    model.get_layer('conv2').set_weights(conv2_weights)

    time_history = TimeHistory()
    history = keras.callbacks.History()

    def schedule(epoch):
        if epoch < 2:
            return 0.005
        elif epoch < 10:
            return 0.002
        elif epoch < 40:
            return 0.001
        elif epoch < 60:
            return 0.0005
        elif epoch < 80:
            return 0.0001
        else:
            return 0.00005

    rate_schedule = keras.callbacks.LearningRateScheduler(schedule)

    print('Using real-time data augmentation.')

    time_history = TimeHistory()
    history = keras.callbacks.History()
    # Fit the model on the batches generated by datagen.flow().
    model.fit(x_train, y_train,
    								 batch_size=32,
    					
    					epochs=10, callbacks=[history, time_history, rate_schedule],
    					validation_split=0.05)

    np.savez('layer3_mnist_results.npz', acc=history.history['acc'], loss=history.history['loss'],
    		val_acc=history.history['val_acc'], val_loss=history.history['val_loss'],
    		times=time_history.times, epoch_times=time_history.epoch_times)

    conv3_weights = model.get_layer('conv3').get_weights()

    return conv3_weights,model