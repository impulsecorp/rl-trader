{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using cuDNN version 7103 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 1080 (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "%pylab inline \n",
    "\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "from collections import Counter\n",
    "import time\n",
    "import progressbar as pb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "import numba as nb\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = 'LunarLander-v2'\n",
    "CONTINUOUS = False\n",
    "\n",
    "EPISODES = 100000\n",
    "\n",
    "LOSS_CLIPPING = 0.2 # Only implemented clipping for the surrogate loss, paper said it was best\n",
    "EPOCHS = 10\n",
    "NOISE = 1.0 # Exploration noise\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "BUFFER_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "NUM_ACTIONS = 4\n",
    "NUM_STATE = 8\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LAYERS = 2\n",
    "ENTROPY_LOSS = 1e-3\n",
    "LR = 1e-4 # Lower lr stabilises training greatly\n",
    "\n",
    "DUMMY_ACTION, DUMMY_VALUE = np.zeros((1, NUM_ACTIONS)), np.zeros((1, 1))\n",
    "\n",
    "\n",
    "@nb.jit\n",
    "def exponential_average(old, new, b1):\n",
    "    return old * b1 + (1-b1) * new\n",
    "\n",
    "\n",
    "def proximal_policy_optimization_loss(advantage, old_prediction):\n",
    "    def loss(y_true, y_pred):\n",
    "        prob = y_true * y_pred\n",
    "        old_prob = y_true * old_prediction\n",
    "        r = prob/(old_prob + 1e-10)\n",
    "        return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantage) + ENTROPY_LOSS * -(prob * K.log(prob + 1e-10)))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def proximal_policy_optimization_loss_continuous(advantage, old_prediction):\n",
    "    def loss(y_true, y_pred):\n",
    "        var = K.square(NOISE)\n",
    "        pi = 3.1415926\n",
    "        denom = K.sqrt(2 * pi * var)\n",
    "        prob_num = K.exp(- K.square(y_true - y_pred) / (2 * var))\n",
    "        old_prob_num = K.exp(- K.square(y_true - old_prediction) / (2 * var))\n",
    "\n",
    "        prob = prob_num/denom\n",
    "        old_prob = old_prob_num/denom\n",
    "        r = prob/(old_prob + 1e-10)\n",
    "\n",
    "        return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantage))\n",
    "    return loss\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.critic = self.build_critic()\n",
    "        if CONTINUOUS is False:\n",
    "            self.actor = self.build_actor()\n",
    "        else:\n",
    "            self.actor = self.build_actor_continuous()\n",
    "\n",
    "        self.env = gym.make(ENV)\n",
    "        print(self.env.action_space, 'action_space', self.env.observation_space, 'observation_space')\n",
    "        self.episode = 0\n",
    "        self.observation = self.env.reset()\n",
    "        self.val = False\n",
    "        self.reward = []\n",
    "        self.reward_over_time = []\n",
    "        self.name = self.get_name()\n",
    "        self.writer = SummaryWriter(self.name)\n",
    "        self.gradient_steps = 0\n",
    "\n",
    "    def get_name(self):\n",
    "        name = 'AllRuns/'\n",
    "        if CONTINUOUS is True:\n",
    "            name += 'continous/'\n",
    "        else:\n",
    "            name += 'discrete/'\n",
    "        name += ENV\n",
    "        return name\n",
    "\n",
    "\n",
    "    def build_actor(self):\n",
    "        state_input = Input(shape=(NUM_STATE,))\n",
    "        advantage = Input(shape=(1,))\n",
    "        old_prediction = Input(shape=(NUM_ACTIONS,))\n",
    "\n",
    "        x = Dense(HIDDEN_SIZE, activation='tanh')(state_input)\n",
    "        for _ in range(NUM_LAYERS - 1):\n",
    "            x = Dense(HIDDEN_SIZE, activation='tanh')(x)\n",
    "\n",
    "        out_actions = Dense(NUM_ACTIONS, activation='softmax', name='output')(x)\n",
    "\n",
    "        model = Model(inputs=[state_input, advantage, old_prediction], outputs=[out_actions])\n",
    "        model.compile(optimizer=Adam(lr=LR),\n",
    "                      loss=[proximal_policy_optimization_loss(\n",
    "                          advantage=advantage,\n",
    "                          old_prediction=old_prediction)])\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def build_actor_continuous(self):\n",
    "        state_input = Input(shape=(NUM_STATE,))\n",
    "        advantage = Input(shape=(1,))\n",
    "        old_prediction = Input(shape=(NUM_ACTIONS,))\n",
    "\n",
    "        x = Dense(HIDDEN_SIZE, activation='tanh')(state_input)\n",
    "        for _ in range(NUM_LAYERS - 1):\n",
    "            x = Dense(HIDDEN_SIZE, activation='tanh')(x)\n",
    "\n",
    "        out_actions = Dense(NUM_ACTIONS, name='output', activation='tanh')(x)\n",
    "\n",
    "        model = Model(inputs=[state_input, advantage, old_prediction], outputs=[out_actions])\n",
    "        model.compile(optimizer=Adam(lr=LR),\n",
    "                      loss=[proximal_policy_optimization_loss_continuous(\n",
    "                          advantage=advantage,\n",
    "                          old_prediction=old_prediction)])\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def build_critic(self):\n",
    "\n",
    "        state_input = Input(shape=(NUM_STATE,))\n",
    "        x = Dense(HIDDEN_SIZE, activation='tanh')(state_input)\n",
    "        for _ in range(NUM_LAYERS - 1):\n",
    "            x = Dense(HIDDEN_SIZE, activation='tanh')(x)\n",
    "\n",
    "        out_value = Dense(1)(x)\n",
    "\n",
    "        model = Model(inputs=[state_input], outputs=[out_value])\n",
    "        model.compile(optimizer=Adam(lr=LR), loss='mse')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def reset_env(self):\n",
    "        self.episode += 1\n",
    "        if self.episode % 100 == 0:\n",
    "            self.val = True\n",
    "        else:\n",
    "            self.val = False\n",
    "        self.observation = self.env.reset()\n",
    "        self.reward = []\n",
    "\n",
    "    def get_action(self):\n",
    "        p = self.actor.predict([self.observation.reshape(1, NUM_STATE), DUMMY_VALUE, DUMMY_ACTION])\n",
    "        if self.val is False:\n",
    "            action = np.random.choice(NUM_ACTIONS, p=np.nan_to_num(p[0]))\n",
    "        else:\n",
    "            action = np.argmax(p[0])\n",
    "        action_matrix = np.zeros(NUM_ACTIONS)\n",
    "        action_matrix[action] = 1\n",
    "        return action, action_matrix, p\n",
    "\n",
    "    def get_action_continuous(self):\n",
    "        p = self.actor.predict([self.observation.reshape(1, NUM_STATE), DUMMY_VALUE, DUMMY_ACTION])\n",
    "        if self.val is False:\n",
    "            action = action_matrix = p[0] + np.random.normal(loc=0, scale=NOISE, size=p[0].shape)\n",
    "        else:\n",
    "            action = action_matrix = p[0]\n",
    "        return action, action_matrix, p\n",
    "\n",
    "    def transform_reward(self):\n",
    "        if self.val is True:\n",
    "            self.writer.add_scalar('Val episode reward', np.array(self.reward).sum(), self.episode)\n",
    "        else:\n",
    "            self.writer.add_scalar('Episode reward', np.array(self.reward).sum(), self.episode)\n",
    "        for j in range(len(self.reward) - 2, -1, -1):\n",
    "            self.reward[j] += self.reward[j + 1] * GAMMA\n",
    "\n",
    "    def get_batch(self):\n",
    "        batch = [[], [], [], []]\n",
    "\n",
    "        tmp_batch = [[], [], []]\n",
    "        while len(batch[0]) < BUFFER_SIZE:\n",
    "            if CONTINUOUS is False:\n",
    "                action, action_matrix, predicted_action = self.get_action()\n",
    "            else:\n",
    "                action, action_matrix, predicted_action = self.get_action_continuous()\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            self.reward.append(reward)\n",
    "\n",
    "            tmp_batch[0].append(self.observation)\n",
    "            tmp_batch[1].append(action_matrix)\n",
    "            tmp_batch[2].append(predicted_action)\n",
    "            self.observation = observation\n",
    "\n",
    "            if done:\n",
    "                self.transform_reward()\n",
    "                if self.val is False:\n",
    "                    for i in range(len(tmp_batch[0])):\n",
    "                        obs, action, pred = tmp_batch[0][i], tmp_batch[1][i], tmp_batch[2][i]\n",
    "                        r = self.reward[i]\n",
    "                        batch[0].append(obs)\n",
    "                        batch[1].append(action)\n",
    "                        batch[2].append(pred)\n",
    "                        batch[3].append(r)\n",
    "                tmp_batch = [[], [], []]\n",
    "                self.reset_env()\n",
    "\n",
    "        obs, action, pred, reward = np.array(batch[0]), np.array(batch[1]), np.array(batch[2]), np.reshape(np.array(batch[3]), (len(batch[3]), 1))\n",
    "        pred = np.reshape(pred, (pred.shape[0], pred.shape[2]))\n",
    "        return obs, action, pred, reward\n",
    "\n",
    "    def run(self):\n",
    "        while self.episode < EPISODES:\n",
    "            obs, action, pred, reward = self.get_batch()\n",
    "            obs, action, pred, reward = obs[:BUFFER_SIZE], action[:BUFFER_SIZE], pred[:BUFFER_SIZE], reward[:BUFFER_SIZE]\n",
    "            old_prediction = pred\n",
    "            pred_values = self.critic.predict(obs)\n",
    "\n",
    "            advantage = reward - pred_values\n",
    "            # advantage = (advantage - advantage.mean()) / advantage.std()\n",
    "            actor_loss = self.actor.fit([obs, advantage, old_prediction], [action], batch_size=BATCH_SIZE, shuffle=True, epochs=EPOCHS, verbose=False)\n",
    "            critic_loss = self.critic.fit([obs], [reward], batch_size=BATCH_SIZE, shuffle=True, epochs=EPOCHS, verbose=False)\n",
    "            self.writer.add_scalar('Actor loss', actor_loss.history['loss'][-1], self.gradient_steps)\n",
    "            self.writer.add_scalar('Critic loss', critic_loss.history['loss'][-1], self.gradient_steps)\n",
    "\n",
    "            self.gradient_steps += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 18,180\n",
      "Trainable params: 18,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Discrete(4) action_space Box(8,) observation_space\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "GpuElemwise. Input dimension mis-match. Input 1 (indices start at 0) has shape[1] == 1, but the output's size on that axis is 4.\nApply node that caused the error: GpuElemwise{mul,no_inplace}(GpuElemwise{true_div,no_inplace}.0, GpuFromHost<None>.0)\nToposort index: 64\nInputs types: [GpuArrayType<None>(float32, matrix), GpuArrayType<None>(float32, matrix)]\nInputs shapes: [(64, 4), (64, 1)]\nInputs strides: [(16, 4), (4, 4)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[GpuElemwise{Composite{minimum(i0, (clip(i1, i2, i3) * i4))}}[]<gpuarray>(GpuElemwise{mul,no_inplace}.0, GpuElemwise{true_div,no_inplace}.0, GpuArrayConstant{[[0.8]]}, GpuArrayConstant{[[1.2]]}, GpuFromHost<None>.0), GpuElemwise{Composite{((((((i0 * EQ(i1, i2) * i3 * i4) / i5) + ((i6 * AND(GE(i7, i8), LE(i7, i9)) * (i10 - EQ(i1, i2)) * i3 * i4) / i5)) / i11) + (i12 * i13) + ((i12 * i14) / i15)) * i16)}}[(0, 2)]<gpuarray>(GpuArrayConstant{[[-1.]]}, GpuElemwise{Composite{minimum(i0, (clip(i1, i2, i3) * i4))}}[]<gpuarray>.0, GpuElemwise{mul,no_inplace}.0, GpuElemwise{true_div,no_inplace}.0, GpuFromHost<None>.0, GpuElemwise{mul,no_inplace}.0, GpuArrayConstant{[[-1.]]}, GpuElemwise{true_div,no_inplace}.0, GpuArrayConstant{[[0.8]]}, GpuArrayConstant{[[1.2]]}, GpuArrayConstant{[[1.]]}, GpuElemwise{Composite{(i0 + (i1 * i2))}}[(0, 2)]<gpuarray>.0, GpuElemwise{Composite{((i0 * i1) / i2)}}[]<gpuarray>.0, GpuElemwise{log,no_inplace}.0, GpuElemwise{mul,no_inplace}.0, GpuElemwise{add,no_inplace}.0, GpuFromHost<None>.0)]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-0e5f96c39455>\", line 1, in <module>\n    ag = Agent()\n  File \"<ipython-input-3-eecd00051da1>\", line 58, in __init__\n    self.actor = self.build_actor()\n  File \"<ipython-input-3-eecd00051da1>\", line 98, in build_actor\n    old_prediction=old_prediction)])\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 342, in compile\n    sample_weight, mask)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\", line 404, in weighted\n    score_array = fn(y_true, y_pred)\n  File \"<ipython-input-3-eecd00051da1>\", line 34, in loss\n    return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantage) + ENTROPY_LOSS * -(prob * K.log(prob + 1e-10)))\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: GpuElemwise. Input dimension mis-match. Input 1 (indices start at 0) has shape[1] == 1, but the output's size on that axis is 4.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0e5f96c39455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-eecd00051da1>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0madvantage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;31m# advantage = (advantage - advantage.mean()) / advantage.std()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_prediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m             \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Actor loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    918\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/gof/link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    690\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: GpuElemwise. Input dimension mis-match. Input 1 (indices start at 0) has shape[1] == 1, but the output's size on that axis is 4.\nApply node that caused the error: GpuElemwise{mul,no_inplace}(GpuElemwise{true_div,no_inplace}.0, GpuFromHost<None>.0)\nToposort index: 64\nInputs types: [GpuArrayType<None>(float32, matrix), GpuArrayType<None>(float32, matrix)]\nInputs shapes: [(64, 4), (64, 1)]\nInputs strides: [(16, 4), (4, 4)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[GpuElemwise{Composite{minimum(i0, (clip(i1, i2, i3) * i4))}}[]<gpuarray>(GpuElemwise{mul,no_inplace}.0, GpuElemwise{true_div,no_inplace}.0, GpuArrayConstant{[[0.8]]}, GpuArrayConstant{[[1.2]]}, GpuFromHost<None>.0), GpuElemwise{Composite{((((((i0 * EQ(i1, i2) * i3 * i4) / i5) + ((i6 * AND(GE(i7, i8), LE(i7, i9)) * (i10 - EQ(i1, i2)) * i3 * i4) / i5)) / i11) + (i12 * i13) + ((i12 * i14) / i15)) * i16)}}[(0, 2)]<gpuarray>(GpuArrayConstant{[[-1.]]}, GpuElemwise{Composite{minimum(i0, (clip(i1, i2, i3) * i4))}}[]<gpuarray>.0, GpuElemwise{mul,no_inplace}.0, GpuElemwise{true_div,no_inplace}.0, GpuFromHost<None>.0, GpuElemwise{mul,no_inplace}.0, GpuArrayConstant{[[-1.]]}, GpuElemwise{true_div,no_inplace}.0, GpuArrayConstant{[[0.8]]}, GpuArrayConstant{[[1.2]]}, GpuArrayConstant{[[1.]]}, GpuElemwise{Composite{(i0 + (i1 * i2))}}[(0, 2)]<gpuarray>.0, GpuElemwise{Composite{((i0 * i1) / i2)}}[]<gpuarray>.0, GpuElemwise{log,no_inplace}.0, GpuElemwise{mul,no_inplace}.0, GpuElemwise{add,no_inplace}.0, GpuFromHost<None>.0)]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-0e5f96c39455>\", line 1, in <module>\n    ag = Agent()\n  File \"<ipython-input-3-eecd00051da1>\", line 58, in __init__\n    self.actor = self.build_actor()\n  File \"<ipython-input-3-eecd00051da1>\", line 98, in build_actor\n    old_prediction=old_prediction)])\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 342, in compile\n    sample_weight, mask)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\", line 404, in weighted\n    score_array = fn(y_true, y_pred)\n  File \"<ipython-input-3-eecd00051da1>\", line 34, in loss\n    return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantage) + ENTROPY_LOSS * -(prob * K.log(prob + 1e-10)))\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "ag = Agent()\n",
    "ag.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the market data\n",
    "input_source = np.load(open('data_spy.npy','rb'))\n",
    "to_predict = np.load(open('data_spy_targets.npy','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_source.shape, to_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = to_predict[3,:].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(to_predict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_source = input_source.T\n",
    "input_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(input_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.matshow(corr)\n",
    "plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "plt.yticks(range(len(corr.columns)), corr.columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "bars_per_episode = 1000\n",
    "winlen = 10\n",
    "class TradingEnv(gym.Env):\n",
    "    \n",
    "    \"\"\" This gym implements a simple trading environment for reinforcement learning. \"\"\"\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete( 3 )\n",
    "        self.observation_space= spaces.Box( #np.min(input_source, axis=0), \n",
    "                                            #np.max(input_source, axis=0)\n",
    "                                            np.ones((winlen*input_source.shape[1], ))*-999999, \n",
    "                                            np.ones((winlen*input_source.shape[1], ))*999999, \n",
    "                                          )\n",
    "        self.reset()\n",
    "        \n",
    "    def _configure(self, display=None):\n",
    "        self.display = display\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        #assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\n",
    "        \n",
    "        if self.idx < self.end_idx:\n",
    "            self.idx += 1\n",
    "            done = False\n",
    "        else:\n",
    "            done = True\n",
    "        \n",
    "        info = {}\n",
    "        \n",
    "        observation = input_source[self.idx - winlen : self.idx, :].reshape(-1)\n",
    "        reward = 0\n",
    "        \n",
    "        # execute the action and get the reward\n",
    "        if action == 0 and self.position == 0: # buy \n",
    "            self.position = -1\n",
    "            self.open_idx = self.idx\n",
    "        if action == 1 and self.position == 0: # sell\n",
    "            self.position = 1\n",
    "            self.open_idx = self.idx\n",
    "        if action == 2 or ((self.position==0) and ((self.idx - self.open_idx) > 8)): # close\n",
    "            if self.position == -1: # long\n",
    "                reward = (to_predict[self.idx] - to_predict[self.open_idx])*10000\n",
    "            elif self.position == 1: # short\n",
    "                reward = (to_predict[self.open_idx] - to_predict[self.idx])*10000\n",
    "            else:\n",
    "                reward = 0\n",
    "            self.position = 0\n",
    "        \n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        # reset and return first observation\n",
    "        self.idx = np.random.randint(0, input_source.shape[0] - bars_per_episode - winlen)\n",
    "        self.end_idx = self.idx + bars_per_episode\n",
    "        self.position = 0\n",
    "        self.open_idx = 0\n",
    "        return input_source[self.idx - winlen : self.idx, :].reshape(-1)\n",
    "    \n",
    "    def _render(self, mode='human', close=False):\n",
    "        #... TODO\n",
    "        pass        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "env = TradingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(env.action_space.n, activation='softmax'))\n",
    "\n",
    "memory = SequentialMemory(limit=20000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, \n",
    "               nb_actions=env.action_space.n, \n",
    "               memory=memory, \n",
    "               nb_steps_warmup=10,\n",
    "               enable_double_dqn=True, \n",
    "               enable_dueling_network=True, \n",
    "               dueling_type='avg', \n",
    "               target_model_update=1e-2, \n",
    "               policy=policy)\n",
    "dqn.compile(Adam(lr=0.002), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training is here\n",
    "h = dqn.fit(env, nb_steps=150000, nb_max_episode_steps=bars_per_episode, visualize=False, verbose=1)\n",
    "rewards = h.history['episode_reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(rewards);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the behavior for one random episode\n",
    "observation = env.reset()\n",
    "done = False\n",
    "navs = []\n",
    "while not done:\n",
    "    action = dqn.forward(observation)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    navs.append(reward)\n",
    "\n",
    "kl = []\n",
    "t = 0\n",
    "for n in navs:\n",
    "    t += n\n",
    "    kl.append(t)\n",
    "plot(kl);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the likelihood of success for any given episode\n",
    "l = 1000\n",
    "krl = []\n",
    "p = pb.ProgressBar(max_value=l)\n",
    "for i in range(l):\n",
    "    p.update(i)\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    navs = []\n",
    "    while not done:\n",
    "        action = dqn.forward(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        navs.append(reward)\n",
    "    krl.append(sum(navs))\n",
    "p.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "krl = array(krl)\n",
    "print('Profit likelihood: %3.3f%%' % (100*(sum(krl > 0) / len(krl))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
