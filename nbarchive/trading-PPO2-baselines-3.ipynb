{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "from collections import Counter\n",
    "import time\n",
    "import progressbar as pb\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil\n",
    "from empyrical import sortino_ratio, calmar_ratio, omega_ratio\n",
    "from stable_baselines.common.policies import MlpPolicy, MlpLstmPolicy, ActorCriticPolicy, FeedForwardPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv, VecEnv, VecEnvWrapper\n",
    "from stable_baselines import A2C, PPO2\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "import tensorflow as tf\n",
    "from trading_env import TradingEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the market data\n",
    "input_source = np.load(open('data_spy.npy','rb'))\n",
    "to_predict = np.load(open('data_spy_targets.npy','rb'))\n",
    "\n",
    "to_predict = to_predict[3,:].reshape(-1)\n",
    "\n",
    "input_source = input_source.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_orig = np.copy(input_source)\n",
    "cp = int(0.8*len(input_source))\n",
    "test_input_source = input_source[cp:, :]\n",
    "test_to_predict = to_predict[cp:]\n",
    "input_source = input_source[0:cp, :]\n",
    "to_predict = to_predict[0:cp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars_per_episode = 1000\n",
    "winlen = 1\n",
    "traded_amt = 10\n",
    "initial_balance = 10000\n",
    "commission = 0\n",
    "slippage = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocess environment\n",
    "#n_cpu = 16\n",
    "#env = SubprocVecEnv([lambda: TradingEnv(input_source, to_predict,\n",
    "#                                         winlen=winlen, bars_per_episode=bars_per_episode, traded_amt=traded_amt, initial_balance=initial_balance,\n",
    "#                                         commission=commission, slippage=slippage) for i in range(n_cpu)])\n",
    "env = TradingEnv(input_source, to_predict,\n",
    "                 winlen=winlen, bars_per_episode=bars_per_episode, traded_amt=traded_amt, initial_balance=initial_balance,\n",
    "                 commission=commission, slippage=slippage\n",
    "                 )\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "t = 0\n",
    "# Custom MLP policy of two layers of size 32 each with tanh activation function\n",
    "policy_kwargs = dict(act_fun=tf.nn.relu, net_arch=[64, 128, 64])\n",
    "\n",
    "#[shutil.rmtree('/home/peter/tblog/'+x) for x in os.listdir('/home/peter/tblog/') if x]\n",
    "model = PPO2(MlpPolicy, env, n_steps=30, verbose=0, nminibatches=1, policy_kwargs=policy_kwargs, \n",
    "             tensorboard_log='/home/peter/tblog')\n",
    "try:\n",
    "    model.learn(total_timesteps=1_000_000)\n",
    "    model.save(\"ppo2_trading\")\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnv(test_input_source, test_to_predict,\n",
    "                 winlen=winlen, bars_per_episode=bars_per_episode, traded_amt=traded_amt,\n",
    "                 commission=commission, slippage=slippage\n",
    "                 )\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# visualize the behavior for one random episode\n",
    "bars_per_episode = 1000\n",
    "\n",
    "nstate = model.initial_state  # get the initial state vector for the reccurent network\n",
    "#dones = np.zeros(nstate.shape[0])  # set all environment to not done\n",
    "nstate=None\n",
    "\n",
    "observation = env.envs[0].reset()#env.reset()\n",
    "done = False\n",
    "navs = []\n",
    "acts = []\n",
    "for i in tqdm(range(bars_per_episode)):\n",
    "    action, nstate = model.predict([observation], state=nstate)\n",
    "    acts.append(action)\n",
    "    observation, reward, done, info = env.envs[0].step(action)#env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "    navs.append(env.get_attr('balance')[0])\n",
    "\n",
    "\n",
    "kl = []\n",
    "t = 0\n",
    "for n in np.diff(np.vstack(navs).reshape(-1)):\n",
    "    t = t + n\n",
    "    kl.append(t)\n",
    "plot(kl);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the likelihood of success for any given episode\n",
    "try:\n",
    "    l = 2500\n",
    "\n",
    "    krl = []\n",
    "    p = pb.ProgressBar(max_value=l)\n",
    "    for i in range(l):\n",
    "        p.update(i)\n",
    "        observation = env.envs[0].reset()\n",
    "        done = False\n",
    "        navs = []\n",
    "        for i in (range(bars_per_episode)):\n",
    "            action, nstate = model.predict([observation], state=nstate)\n",
    "            acts.append(action)\n",
    "            observation, reward, done, info = env.envs[0].step(action)#env.step(action)\n",
    "            navs.append(env.get_attr('balance')[0])\n",
    "        krl.append(sum(navs))\n",
    "    p.finish()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "krl = np.array(krl)\n",
    "print('Profit likelihood: %3.3f%%' % (100*(sum(krl > 0) / len(krl))))\n",
    "\n",
    "hist(krl);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
