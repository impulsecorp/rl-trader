{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "from collections import Counter\n",
    "import time\n",
    "import progressbar as pb\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, LSTM, Reshape, Dropout, Input, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.cem import CEMAgent \n",
    "from rl.agents.ddpg import DDPGAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory, EpisodeParameterMemory\n",
    "from rl.processors import WhiteningNormalizerProcessor\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empyrical import sortino_ratio, calmar_ratio, omega_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the market data\n",
    "input_source = np.load(open('data_spy.npy','rb'))\n",
    "to_predict = np.load(open('data_spy_targets.npy','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_source.shape, to_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = to_predict[3,:].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(to_predict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_source = input_source.T\n",
    "input_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_source = input_source[int(0.8*len(input_source)):, :]\n",
    "input_source = input_source[0:int(0.8*len(input_source)), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(input_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.matshow(corr)\n",
    "plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "plt.yticks(range(len(corr.columns)), corr.columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "bars_per_episode = 1000\n",
    "winlen = 10\n",
    "class TradingEnv(gym.Env):\n",
    "    \n",
    "    \"\"\" This gym implements a simple trading environment for reinforcement learning. \"\"\"\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete( 3 )\n",
    "        self.observation_space= spaces.Box( #np.min(input_source, axis=0), \n",
    "                                            #np.max(input_source, axis=0)\n",
    "                                            np.ones((winlen,input_source.shape[1], ))*-999999, \n",
    "                                            np.ones((winlen,input_source.shape[1], ))*999999, \n",
    "                                          )\n",
    "        self.reset()\n",
    "        \n",
    "    def _configure(self, display=None):\n",
    "        self.display = display\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        #assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\n",
    "        \n",
    "        if (self.idx < self.end_idx) and (self.balance > 0):\n",
    "            self.idx += 1\n",
    "            done = False\n",
    "        else:\n",
    "            done = True\n",
    "        \n",
    "        info = {}\n",
    "        \n",
    "        observation = input_source[self.idx - winlen : self.idx, :]\n",
    "        \n",
    "        # execute the action and get the reward\n",
    "        if np.argmax(action) == 0 and self.position == 0: # buy \n",
    "            self.position = -1\n",
    "            self.open_idx = self.idx\n",
    "        if np.argmax(action) == 1 and self.position == 0: # sell\n",
    "            self.position = 1\n",
    "            self.open_idx = self.idx\n",
    "        if np.argmax(action) == 2:# or ((self.position==0) and ((self.idx - self.open_idx) > 8)): # close\n",
    "            if self.position == -1: # long\n",
    "                self.balance += (to_predict[self.idx] - to_predict[self.open_idx])*1000\n",
    "            elif self.position == 1: # short\n",
    "                self.balance += (to_predict[self.open_idx] - to_predict[self.idx])*1000\n",
    "            self.position = 0\n",
    "        if np.argmax(action) == 3:\n",
    "            pass\n",
    "        \n",
    "        self.returns.append(self.balance - 1000)\n",
    "        \n",
    "        if len(self.returns) > 5:\n",
    "            reward = sortino_ratio(np.diff(np.array(self.returns[:])))\n",
    "            #print(np.diff(np.array(self.returns[1:])))\n",
    "            #print(reward)\n",
    "            if isnan(reward) or isinf(reward):\n",
    "                reward = 0\n",
    "        else:\n",
    "            reward = 0\n",
    "        self.prev_balance = self.balance\n",
    "        \n",
    "        \n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        # reset and return first observation\n",
    "        self.idx = np.random.randint(winlen+1, input_source.shape[0] - bars_per_episode - winlen)\n",
    "        self.end_idx = self.idx + bars_per_episode\n",
    "        self.position = 0\n",
    "        self.open_idx = 0\n",
    "        self.balance = 1000\n",
    "        self.prev_balance = self.balance\n",
    "        self.returns = []\n",
    "        return input_source[self.idx - winlen : self.idx, :]\n",
    "    \n",
    "    def reset2(self):\n",
    "        # reset and return first observation\n",
    "        self.idx = winlen\n",
    "        self.end_idx = self.idx + bars_per_episode\n",
    "        self.position = 0\n",
    "        self.open_idx = 0\n",
    "        self.balance = 1000\n",
    "        self.prev_balance = self.balance\n",
    "        self.returns = []\n",
    "        return input_source[self.idx - winlen : self.idx, :]\n",
    "    \n",
    "    def _render(self, mode='human', close=False):\n",
    "        #... TODO\n",
    "        pass        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(Reshape(env.observation_space.shape, input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(env.action_space.n, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we build a very simple model.\n",
    "actor = Sequential()\n",
    "actor.add(Reshape(env.observation_space.shape, input_shape=(1,) + env.observation_space.shape))\n",
    "actor.add(LSTM(16))\n",
    "#actor.add(Dropout(0.5))\n",
    "actor.add(Dense(32))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(env.action_space.n, activation='tanh'))\n",
    "print(actor.summary())\n",
    "\n",
    "action_input = Input(shape=(env.action_space.n,), name='action_input')\n",
    "observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n",
    "flattened_observation = Flatten()(observation_input)\n",
    "x = Dense(64)(flattened_observation)\n",
    "x = Activation('relu')(x)\n",
    "x = Concatenate()([x, action_input])\n",
    "x = Dense(64)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "print(critic.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class MyProcessor(WhiteningNormalizerProcessor):\n",
    "#    def process_action(self, action):\n",
    "#        return np.clip(action, -1., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(size=env.action_space.n, theta=.15, mu=0., sigma=.1)\n",
    "agent = DDPGAgent(nb_actions=env.action_space.n, actor=actor, critic=critic, critic_action_input=action_input,\n",
    "                  memory=memory, nb_steps_warmup_critic=10, nb_steps_warmup_actor=10,\n",
    "                  random_process=random_process, gamma=.99, target_model_update=1e-3,\n",
    "                  processor=WhiteningNormalizerProcessor()\n",
    "                 )\n",
    "agent.compile([Adam(lr=1e-4), Adam(lr=1e-3)], metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training is here\n",
    "h = agent.fit(env, nb_steps=50000, nb_max_episode_steps=bars_per_episode, visualize=False, verbose=1)\n",
    "rewards = h.history['episode_reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(rewards);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the behavior for one random episode\n",
    "bars_per_episode = 100000\n",
    "input_source = test_input_source \n",
    "\n",
    "observation = env.reset()\n",
    "done = False\n",
    "navs = []\n",
    "while not done:\n",
    "    action = agent.forward(observation)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    navs.append(reward)\n",
    "\n",
    "kl = []\n",
    "t = 0\n",
    "for n in navs:\n",
    "    t += n\n",
    "    kl.append(t)\n",
    "plot(kl);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "navs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate the likelihood of success for any given episode\n",
    "l = 100\n",
    "krl = []\n",
    "p = pb.ProgressBar(max_value=l)\n",
    "for i in range(l):\n",
    "    p.update(i)\n",
    "    observation = env.reset2()\n",
    "    done = False\n",
    "    navs = []\n",
    "    while not done:\n",
    "        action = agent.forward(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        navs.append(reward)\n",
    "    krl.append(sum(navs))\n",
    "p.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "krl = array(krl)\n",
    "print('Profit likelihood: %3.3f%%' % (100*(sum(krl > 0) / len(krl))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
