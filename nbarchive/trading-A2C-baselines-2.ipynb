{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "from collections import Counter\n",
    "import time\n",
    "import progressbar as pb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empyrical import sortino_ratio, calmar_ratio, omega_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the market data\n",
    "input_source = np.load(open('data_spy.npy','rb'))\n",
    "to_predict = np.load(open('data_spy_targets.npy','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_source.shape, to_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = to_predict[3,:].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(to_predict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_source = input_source.T\n",
    "input_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_source = input_source[int(0.8*len(input_source)):, :]\n",
    "input_source = input_source[0:int(0.8*len(input_source)), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(input_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.matshow(corr)\n",
    "plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "plt.yticks(range(len(corr.columns)), corr.columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "bars_per_episode = 1000\n",
    "winlen = 1\n",
    "class TradingEnv(gym.Env):\n",
    "    \n",
    "    \"\"\" This gym implements a simple trading environment for reinforcement learning. \"\"\"\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete( 3 )\n",
    "        self.observation_space= spaces.Box( #np.min(input_source, axis=0), \n",
    "                                            #np.max(input_source, axis=0)\n",
    "                                            np.ones((winlen*input_source.shape[1], ))*-999999, \n",
    "                                            np.ones((winlen*input_source.shape[1], ))*999999, \n",
    "                                          )\n",
    "        self.reset()\n",
    "        \n",
    "    def _configure(self, display=None):\n",
    "        self.display = display\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        #assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\n",
    "        \n",
    "        if (self.idx < self.end_idx) and (self.balance > 0):\n",
    "            self.idx += 1\n",
    "            done = False\n",
    "        else:\n",
    "            done = True\n",
    "        \n",
    "        info = {}\n",
    "        \n",
    "        observation = input_source[self.idx - winlen : self.idx, :].reshape(-1)\n",
    "        \n",
    "        # execute the action and get the reward\n",
    "        if action == 0 and self.position == 0: # buy \n",
    "            self.position = -1\n",
    "            self.open_idx = self.idx\n",
    "        if action == 1 and self.position == 0: # sell\n",
    "            self.position = 1\n",
    "            self.open_idx = self.idx\n",
    "        if action == 2:# or ((self.position==0) and ((self.idx - self.open_idx) > 8)): # close\n",
    "            if self.position == -1: # long\n",
    "                self.balance += (to_predict[self.idx] - to_predict[self.open_idx])*1000\n",
    "            elif self.position == 1: # short\n",
    "                self.balance += (to_predict[self.open_idx] - to_predict[self.idx])*1000\n",
    "            self.position = 0\n",
    "        if action == 3:\n",
    "            pass\n",
    "        \n",
    "        self.returns.append(self.balance - 1000)\n",
    "        \n",
    "        if len(self.returns) > 5:\n",
    "            reward = sortino_ratio(np.diff(np.array(self.returns[:])))\n",
    "            #print(np.diff(np.array(self.returns[1:])))\n",
    "            #print(reward)\n",
    "            if isnan(reward) or isinf(reward):\n",
    "                reward = 0\n",
    "        else:\n",
    "            reward = 0\n",
    "        self.prev_balance = self.balance\n",
    "        \n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        # reset and return first observation\n",
    "        self.idx = np.random.randint(winlen, input_source.shape[0] - bars_per_episode)\n",
    "        self.end_idx = self.idx + bars_per_episode\n",
    "        self.position = 0\n",
    "        self.open_idx = 0\n",
    "        self.balance = 1000\n",
    "        self.prev_balance = self.balance\n",
    "        self.returns = []\n",
    "        return input_source[self.idx - winlen : self.idx, :].reshape(-1)\n",
    "    \n",
    "    def _render(self, mode='human', close=False):\n",
    "        #... TODO\n",
    "        pass        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy, MlpLstmPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv, VecEnv, VecEnvWrapper\n",
    "from stable_baselines import A2C, PPO2\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "# multiprocess environment\n",
    "#n_cpu = 16\n",
    "#env = SubprocVecEnv([lambda: TradingEnv() for i in range(n_cpu)])\n",
    "env = TradingEnv()\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = A2C(MlpLstmPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=100000)\n",
    "model.save(\"a2c_trading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = A2C.load(\"a2c_trading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnv()\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# visualize the behavior for one random episode\n",
    "bars_per_episode = 10000\n",
    "#input_source = test_input_source \n",
    "\n",
    "states = model.initial_state  # get the initial state vector for the reccurent network\n",
    "dones = np.zeros(states.shape[0])  # set all environment to not done\n",
    "\n",
    "observation = env.reset()\n",
    "env.envs[0].balance = 1000\n",
    "done = False\n",
    "navs = []\n",
    "for i in tqdm(range(bars_per_episode)):\n",
    "    action, *_ = model.predict(observation, states, dones) \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    navs.append(env.envs[0].balance)\n",
    "\n",
    "kl = []\n",
    "t = 0\n",
    "for n in vstack(navs):\n",
    "    t = t + n\n",
    "    kl.append(t)\n",
    "plot(kl);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate the likelihood of success for any given episode\n",
    "l = 1000\n",
    "krl = []\n",
    "p = pb.ProgressBar(max_value=l)\n",
    "for i in range(l):\n",
    "    p.update(i)\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    navs = []\n",
    "    while not done:\n",
    "        action = model.predict(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        navs.append(env.envs[0].balance)\n",
    "    krl.append(sum(navs))\n",
    "p.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "krl = array(krl)\n",
    "print('Profit likelihood: %3.3f%%' % (100*(sum(krl > 0) / len(krl))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(krl, 40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "navs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
