{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "from collections import Counter\n",
    "import time\n",
    "import progressbar as pb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, LSTM, Reshape, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent \n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the market data\n",
    "input_source = np.load(open('data_spy.npy','rb'))\n",
    "to_predict = np.load(open('data_spy_targets.npy','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_source.shape, to_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = to_predict[3,:].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(to_predict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_source = input_source.T\n",
    "input_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(input_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.matshow(corr)\n",
    "plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "plt.yticks(range(len(corr.columns)), corr.columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "bars_per_episode = 1000\n",
    "winlen = 10\n",
    "class TradingEnv(gym.Env):\n",
    "    \n",
    "    \"\"\" This gym implements a simple trading environment for reinforcement learning. \"\"\"\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete( 3 )\n",
    "        self.observation_space= spaces.Box( #np.min(input_source, axis=0), \n",
    "                                            #np.max(input_source, axis=0)\n",
    "                                            np.ones((winlen,input_source.shape[1], ))*-999999, \n",
    "                                            np.ones((winlen,input_source.shape[1], ))*999999, \n",
    "                                          )\n",
    "        self.reset()\n",
    "        \n",
    "    def _configure(self, display=None):\n",
    "        self.display = display\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        #assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\n",
    "        \n",
    "        if (self.idx < self.end_idx) and (self.balance > 0):\n",
    "            self.idx += 1\n",
    "            done = False\n",
    "        else:\n",
    "            done = True\n",
    "        \n",
    "        info = {}\n",
    "        \n",
    "        observation = input_source[self.idx - winlen : self.idx, :]\n",
    "        \n",
    "        # execute the action and get the reward\n",
    "        if action == 0 and self.position == 0: # buy \n",
    "            self.position = -1\n",
    "            self.open_idx = self.idx\n",
    "        if action == 1 and self.position == 0: # sell\n",
    "            self.position = 1\n",
    "            self.open_idx = self.idx\n",
    "        if action == 2 or ((self.position==0) and ((self.idx - self.open_idx) > 8)): # close\n",
    "            if self.position == -1: # long\n",
    "                self.balance += (to_predict[self.idx] - to_predict[self.open_idx])*1000\n",
    "            elif self.position == 1: # short\n",
    "                self.balance += (to_predict[self.open_idx] - to_predict[self.idx])*1000\n",
    "            self.position = 0\n",
    "        if action == 3:\n",
    "            pass\n",
    "        \n",
    "        reward = self.balance - self.prev_balance\n",
    "        self.prev_balance = self.balance\n",
    "        \n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        # reset and return first observation\n",
    "        self.idx = np.random.randint(0, input_source.shape[0] - bars_per_episode - winlen)\n",
    "        self.end_idx = self.idx + bars_per_episode\n",
    "        self.position = 0\n",
    "        self.open_idx = 0\n",
    "        self.balance = 1000\n",
    "        self.prev_balance = self.balance\n",
    "        return input_source[self.idx - winlen : self.idx, :]\n",
    "    \n",
    "    def _render(self, mode='human', close=False):\n",
    "        #... TODO\n",
    "        pass        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Reshape(env.observation_space.shape, input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(env.action_space.n, activation='softmax'))\n",
    "\n",
    "memory = SequentialMemory(limit=10000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, \n",
    "               nb_actions=env.action_space.n, \n",
    "               memory=memory, \n",
    "               nb_steps_warmup=10,\n",
    "               enable_double_dqn=True, \n",
    "               enable_dueling_network=True, \n",
    "               dueling_type='avg', \n",
    "               target_model_update=1e-2, \n",
    "               policy=policy)\n",
    "dqn.compile(Adam(lr=0.002), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training is here\n",
    "h = dqn.fit(env, nb_steps=300000, nb_max_episode_steps=bars_per_episode, visualize=False, verbose=1)\n",
    "rewards = h.history['episode_reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(rewards);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the behavior for one random episode\n",
    "observation = env.reset()\n",
    "done = False\n",
    "navs = []\n",
    "while not done:\n",
    "    action = dqn.forward(observation)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    navs.append(reward)\n",
    "\n",
    "kl = []\n",
    "t = 0\n",
    "for n in navs:\n",
    "    t += n\n",
    "    kl.append(t)\n",
    "plot(kl);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate the likelihood of success for any given episode\n",
    "l = 1000\n",
    "krl = []\n",
    "p = pb.ProgressBar(max_value=l)\n",
    "for i in range(l):\n",
    "    p.update(i)\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    navs = []\n",
    "    while not done:\n",
    "        action = dqn.forward(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        navs.append(reward)\n",
    "    krl.append(sum(navs))\n",
    "p.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "krl = array(krl)\n",
    "print('Profit likelihood: %3.3f%%' % (100*(sum(krl > 0) / len(krl))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
